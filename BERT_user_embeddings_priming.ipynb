{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqQfCdPk-yO"
      },
      "source": [
        "#Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlV1Au7qE8xH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCd6mm5S4Z6U",
        "outputId": "760ee3e7-c0bb-4995-d67a-d846c11d5621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.7/dist-packages (3.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from rpy2) (2.11.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from rpy2) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from rpy2) (2022.1)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from rpy2) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->rpy2) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "!pip install rpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGxc0tQ1wg_K",
        "outputId": "069f4c22-e8d6-4a7f-96bc-3d22a1c8fc49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrJxCC3SwBhO",
        "outputId": "a56bd3d0-378b-4bd2-e1cc-240585834ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh8Z-6gRwjZ_",
        "outputId": "5494d5e0-c2ea-4b2d-a16d-85efa975b03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "Python implementation: CPython\n",
            "Python version       : 3.7.13\n",
            "IPython version      : 5.5.0\n",
            "\n",
            "numpy       : 1.21.6\n",
            "pandas      : 1.3.5\n",
            "torch       : 1.11.0+cu113\n",
            "transformers: 4.20.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r30ag7chkWz7"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT8BX9Ch5rPP",
        "outputId": "a851f6a8-b902-4faf-9862-ef1dfa9f2d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "!%%cache\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/SP\")\n",
        "df_user_embeddings_sarcastic = pd.read_csv('/content/drive/MyDrive/SP/user_embeddings_sarcastic_beginning.csv')\n",
        "df_user_embeddings_non_sarcastic = pd.read_csv('/content/drive/MyDrive/SP/user_embeddings_non_sarcastic_beginning.csv')\n",
        "\n",
        "df_user_embeddings_sarcastic['label'] = 1\n",
        "df_user_embeddings_non_sarcastic['label'] = 0\n",
        "\n",
        "data = pd.concat([df_user_embeddings_sarcastic, df_user_embeddings_non_sarcastic], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "r0a22-MVZZwj",
        "outputId": "5c4f211a-1655-4789-dd5c-38116c07f825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   user_id        sar_id  \\\n",
              "0       707672778867130368  1.200914e+18   \n",
              "1                 51123724  1.190708e+18   \n",
              "2       758087956754161664  1.197762e+18   \n",
              "3       758087956754161664  1.187123e+18   \n",
              "4                228269677  1.197700e+18   \n",
              "...                    ...           ...   \n",
              "23546  1158675019989487616  1.195250e+18   \n",
              "23547           2877956828  1.199358e+18   \n",
              "23548           2376518743  1.199311e+18   \n",
              "23549           2958107288  1.195407e+18   \n",
              "23550            454099221  1.195477e+18   \n",
              "\n",
              "                                                sar_text  label  \n",
              "0      [CLS] pick hit divine [SEP] rt saw kamala prot...      1  \n",
              "1      [CLS] guess i'm nosugarnovember!! challenge th...      1  \n",
              "2      [CLS] isnt great? its open. taxpayer funded......      1  \n",
              "3      [CLS] joking. [SEP] wow [SEP] fiction..... [SE...      1  \n",
              "4      [CLS] hoyer didn't. maybe in. [SEP] unlocked \"...      1  \n",
              "...                                                  ...    ...  \n",
              "23546  [CLS]  [SEP] stop :face_with_tears_of_joy::fac...      0  \n",
              "23547  [CLS] 1. sure written, sanders definitely look...      0  \n",
              "23548  [CLS] service growingso internet connection! :...      0  \n",
              "23549  [CLS] :thinking_face: [SEP] rt scientists show...      0  \n",
              "23550  [CLS] wanna feel old?? no. 34 turns 50 sunday....      0  \n",
              "\n",
              "[23551 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83f249f3-0cc2-4133-9e00-5f51eff7ea59\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>sar_id</th>\n",
              "      <th>sar_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>707672778867130368</td>\n",
              "      <td>1.200914e+18</td>\n",
              "      <td>[CLS] pick hit divine [SEP] rt saw kamala prot...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51123724</td>\n",
              "      <td>1.190708e+18</td>\n",
              "      <td>[CLS] guess i'm nosugarnovember!! challenge th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>758087956754161664</td>\n",
              "      <td>1.197762e+18</td>\n",
              "      <td>[CLS] isnt great? its open. taxpayer funded......</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>758087956754161664</td>\n",
              "      <td>1.187123e+18</td>\n",
              "      <td>[CLS] joking. [SEP] wow [SEP] fiction..... [SE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>228269677</td>\n",
              "      <td>1.197700e+18</td>\n",
              "      <td>[CLS] hoyer didn't. maybe in. [SEP] unlocked \"...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23546</th>\n",
              "      <td>1158675019989487616</td>\n",
              "      <td>1.195250e+18</td>\n",
              "      <td>[CLS]  [SEP] stop :face_with_tears_of_joy::fac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23547</th>\n",
              "      <td>2877956828</td>\n",
              "      <td>1.199358e+18</td>\n",
              "      <td>[CLS] 1. sure written, sanders definitely look...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23548</th>\n",
              "      <td>2376518743</td>\n",
              "      <td>1.199311e+18</td>\n",
              "      <td>[CLS] service growingso internet connection! :...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23549</th>\n",
              "      <td>2958107288</td>\n",
              "      <td>1.195407e+18</td>\n",
              "      <td>[CLS] :thinking_face: [SEP] rt scientists show...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23550</th>\n",
              "      <td>454099221</td>\n",
              "      <td>1.195477e+18</td>\n",
              "      <td>[CLS] wanna feel old?? no. 34 turns 50 sunday....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23551 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83f249f3-0cc2-4133-9e00-5f51eff7ea59')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83f249f3-0cc2-4133-9e00-5f51eff7ea59 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83f249f3-0cc2-4133-9e00-5f51eff7ea59');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup and Confing\n"
      ],
      "metadata": {
        "id": "3eb8Jc4qNd2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAwQMmKINaSX",
        "outputId": "6bab716b-6900-43ce-e261-969e2bd57b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzkHiBOpwARW"
      },
      "outputs": [],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFSN4CiwxINU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "237bfdf55b344e78bb77758cd04b6acb",
            "16f9c82134ef4c4fa5636e9ec5ec2620",
            "a94692d114a14c048b09b033b7be974e",
            "e306d588ee5a4724996292c3f2a5bef1",
            "971804bd27b14375a46a3b3a97bc133d",
            "b2ad6349033b43b7bccb4b4a2e87cdc5",
            "77b666be06df44118cda143aed2dafd6",
            "7b95e4a2d50348caad1413270c9566d6",
            "3f372615774140d68722d690a99828b2",
            "57aac605ed9b4bdc9028cdac53f902d2",
            "f19c9189c9674760a9690b2cf955b832",
            "875c6564c23c4c36be6c9b6564ae9133",
            "06968769602b459aae89b9a4d9fcc5c8",
            "a2a158d5f4a44df8becedb66020cefc3",
            "020380022e2149e3851312018bbca75a",
            "e3ea2795cadd4e26895c07b3c5f970ff",
            "b4e4d4406b6241a983eee07d5cfc106a",
            "cc59c2d25a1a42eba83cbedcbdfa8271",
            "a352308384494aba9011dfb3627b9f99",
            "88d2c3c5b5214074a12286765be0ad70",
            "51c1404dfcfb494cad13a5d37d9ec918",
            "73d182e6195a49f98e076ef559569a75",
            "2ffb2ca2e6eb4d8c844ff8e17d984c5c",
            "a76de41fe56a4ecb9268e0fc766a345b",
            "53dbf68acece4adf901d7083ad22d897",
            "b0627e57e0594cd681b6e58635457b1d",
            "b827fffa9146419eae05e9046d812bf9",
            "6a327ca8f49a4078a6990147a1c30fea",
            "36dd68ad6ebc4b01b21c76d87c96a320",
            "24dda418930c4206a01081efcad2c6a2",
            "c510def81f954c0e9d4bc5dbf1ce12bc",
            "911fc58a4777465a9f2d16f75349e7a5",
            "693e5991f8f64949a0dc57375dc3f932"
          ]
        },
        "outputId": "2b6d6349-5f23-4eb0-ca41-ff54d8991002"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "237bfdf55b344e78bb77758cd04b6acb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "875c6564c23c4c36be6c9b6564ae9133"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ffb2ca2e6eb4d8c844ff8e17d984c5c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6g3gHfy2JpF"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4kB5OeW0j6n"
      },
      "outputs": [],
      "source": [
        "class SarcasticTweetsDataset(Dataset):\n",
        "\n",
        "  def __init__(self, sar_texts, targets, tokenizer, max_len):\n",
        "    self.sar_texts = sar_texts\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sar_texts)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    sar_text = str(self.sar_texts[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      sar_text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'sar_text': sar_text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dhjGDcd1SI0"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = SarcasticTweetsDataset(\n",
        "    sar_texts=df.sar_text.to_numpy(),\n",
        "    targets=df.label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBKCmqL91HNT"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac=1)\n",
        "df_user_embeddings = data\n",
        "df_train, df_test = train_test_split(data, test_size=0.1, shuffle=False)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_user_embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiBe0SYcGBQN",
        "outputId": "bacc9fe2-f56b-46ee-803a-2dd49ab6965c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "df_train1, df_test1 = train_test_split(df_user_embeddings, test_size=0.1, shuffle=False)\n",
        "print(df_test1.head())\n",
        "df_val1, df_test1 = train_test_split(df_test1, test_size=0.5, shuffle=False)\n",
        "print(df_test1.head())\n",
        "print('\\n\\n')\n",
        "n_train = math.floor(0.9 * df_user_embeddings['label'].shape[0])\n",
        "dffff = df_user_embeddings[n_train:]\n",
        "n_train = math.floor(0.5*dffff['label'].shape[0])\n",
        "dffff = dffff[n_train:]\n",
        "#n = len(df_user_embeddings) * 0.1\n",
        "#n *= 0.5\n",
        "print(dffff.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYvsa5xoFnqe",
        "outputId": "6630ba1b-9900-49bc-ac7b-098b71f35eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   user_id        sar_id  \\\n",
            "4481   1068449652171841536  1.200039e+18   \n",
            "3716    711535251835629568  1.189917e+18   \n",
            "10837  1057221577144713217  1.196174e+18   \n",
            "6140    943153792861609985  1.189289e+18   \n",
            "9956    945333310934327296  1.194675e+18   \n",
            "\n",
            "                                                sar_text  label  \n",
            "4481   [CLS] whys [SEP] rt 10 european clubs accordin...      1  \n",
            "3716   [CLS] japanese understand meaning victimhood c...      1  \n",
            "10837  [CLS] share message 20 people contact list rec...      1  \n",
            "6140   [CLS] aren't loyal fan point teams flaws [SEP]...      1  \n",
            "9956   [CLS] not, know lawyers? [SEP] youre shit. ts ...      1  \n",
            "                   user_id        sar_id  \\\n",
            "4971             115154507  1.200983e+18   \n",
            "23236            206339890  1.195567e+18   \n",
            "13226            342836475  1.199351e+18   \n",
            "6783            1528548649  1.188642e+18   \n",
            "3600   1014590389251198976  1.192991e+18   \n",
            "\n",
            "                                                sar_text  label  \n",
            "4971   [CLS] its fault, its the system. [SEP] rt walk...      1  \n",
            "23236  [CLS] sex worker [SEP] everybody wanna guns do...      0  \n",
            "13226  [CLS] controversial dumb haha [SEP] it's fun t...      0  \n",
            "6783   [CLS] love woman look [SEP] eats pussy love. [...      1  \n",
            "3600   [CLS] use spoiler warnings? read books. [SEP] ...      1  \n",
            "\n",
            "\n",
            "\n",
            "                   user_id        sar_id  \\\n",
            "4971             115154507  1.200983e+18   \n",
            "23236            206339890  1.195567e+18   \n",
            "13226            342836475  1.199351e+18   \n",
            "6783            1528548649  1.188642e+18   \n",
            "3600   1014590389251198976  1.192991e+18   \n",
            "\n",
            "                                                sar_text  label  \n",
            "4971   [CLS] its fault, its the system. [SEP] rt walk...      1  \n",
            "23236  [CLS] sex worker [SEP] everybody wanna guns do...      0  \n",
            "13226  [CLS] controversial dumb haha [SEP] it's fun t...      0  \n",
            "6783   [CLS] love woman look [SEP] eats pussy love. [...      1  \n",
            "3600   [CLS] use spoiler warnings? read books. [SEP] ...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOuuOGPi1M84",
        "outputId": "69c42cf2-b3b7-41dd-8410-cdb9575df5eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21195, 4), (1178, 4), (1178, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBV-lpwf1ifV",
        "outputId": "13f175b9-21fd-49e4-ab4e-ca30f2ea786f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 512\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpyy5TB_1x_S",
        "outputId": "fae95e4e-d690-4a34-e6a4-a800f90d5227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['sar_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpNze8ZP1zSS",
        "outputId": "eb33f5ec-831f-4bdf-8dfb-20d6a9308ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 512])\n",
            "torch.Size([16, 512])\n",
            "torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEFm3ELt6OD6"
      },
      "source": [
        "#Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4gQhe40B9xE"
      },
      "outputs": [],
      "source": [
        "class SarcasmClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    ob = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(ob.pooler_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZn6lkIo9ey_"
      },
      "outputs": [],
      "source": [
        "class_names = ['positive', 'negative']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVkEGDez9OH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d8412807347b4abcb1ceac36d3b4a3b9",
            "9ae86b8cf23b49e7aed9a6220bdec217",
            "71ad58a7e75548369bd68a0d34d95401",
            "be6358dce9f24a28bcfa36fb6e816e2f",
            "619fbbf748c8499bb5fe84f36b9310af",
            "8d4861ef458e42aba65104315ddb33de",
            "6ee2a79a7ce24ed1ba3589bfbb6c6591",
            "0078bdfa6b854dbaa899ad61e545603b",
            "1e075cef02f54337b37dd0181c06b4cf",
            "d76f4b67a28b4f8da16e0e1d3609c619",
            "04109be4e4a84c4e9e7310fea336fc23",
            "62848be1d268400495f84f6276cd37b5",
            "a966adc18cc643e1b1192d1ad8ed95c6",
            "a6328f0333514af485806319e3b1d4c7",
            "30cbe335cbe14d4da065f3e0cd11f7d0",
            "aa2dca0a32ee43748eeb2036997e5896",
            "b5f7d341e20f4c9b924660d7bb84ca05",
            "68fdd998d68349d48652471fe741f3c4",
            "5c3cd1c748604d9c8e4e77b1da71e5c0",
            "03481f7f2da9468991c3f0403e3bde0a",
            "50e8b2b040e640da8db05acd01dc24a5",
            "1b035aa0cdb14307bf294ae1afd3d0ec"
          ]
        },
        "outputId": "58fd2c10-49b3-4d7e-b838-b3bf436d810b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8412807347b4abcb1ceac36d3b4a3b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62848be1d268400495f84f6276cd37b5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = SarcasmClassifier(len(class_names))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvA4gmEm9uAs",
        "outputId": "48094b36-cf07-411b-c50c-8a50aee35e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 512])\n",
            "torch.Size([16, 512])\n"
          ]
        }
      ],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPU-1Ki_9xrJ",
        "outputId": "b48c4fab-d4c8-46d2-fab4-8c3e8e47698f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5251, 0.4749],\n",
              "        [0.5339, 0.4661],\n",
              "        [0.5172, 0.4828],\n",
              "        [0.5181, 0.4819],\n",
              "        [0.5277, 0.4723],\n",
              "        [0.5204, 0.4796],\n",
              "        [0.5187, 0.4813],\n",
              "        [0.5279, 0.4721],\n",
              "        [0.5316, 0.4684],\n",
              "        [0.5238, 0.4762],\n",
              "        [0.5161, 0.4839],\n",
              "        [0.5306, 0.4694],\n",
              "        [0.5219, 0.4781],\n",
              "        [0.5196, 0.4804],\n",
              "        [0.5407, 0.4593],\n",
              "        [0.5383, 0.4617]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uDCtSxd95V5"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTlmpyrb9yu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20058f12-c9d9-4f96-cf22-91eb479c0b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApnUtL4-B-nY"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE-7PLDjCAY3"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NakwLrugCJ5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4581f712-2d86-4ecf-837a-58ccdb62849f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6231623737767058 accuracy 0.6562396791696155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6004157932223501 accuracy 0.6977928692699491\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5529698694651981 accuracy 0.7250294880868129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6054925950797828 accuracy 0.6952461799660441\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.49481088937453505 accuracy 0.7681056853031375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6409843068670582 accuracy 0.698641765704584\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.434880605413104 accuracy 0.8089643783911299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6783118127165614 accuracy 0.700339558573854\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.37932890849012246 accuracy 0.8428402925218211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7286428633976627 accuracy 0.702037351443124\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.32954291003914377 accuracy 0.870488322717622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8070793768038621 accuracy 0.699490662139219\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3029440258836971 accuracy 0.8835102618542109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8537799417972565 accuracy 0.6969439728353141\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2694912279917384 accuracy 0.8996933238971455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9385344992215569 accuracy 0.6926994906621392\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.25493910049281593 accuracy 0.90828025477707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.0164277622023143 accuracy 0.6850594227504244\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.23665003716172475 accuracy 0.9141306912007549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1069569468900964 accuracy 0.6867572156196944\n",
            "\n",
            "CPU times: user 1h 8min, sys: 16.6 s, total: 1h 8min 17s\n",
            "Wall time: 1h 11min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM8MHtF0UUZH"
      },
      "outputs": [],
      "source": [
        "tmp = history['train_acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWa2Y3BFLij4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5335db70-89ed-41d7-eace-0e34e003a672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6485568760611206"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHhrhEdkLs4m"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  sar_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"sar_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      sar_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return sar_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M-W2I5TL7XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafa55bd-fa12-4f5d-d845-034dd78debf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics and JSON"
      ],
      "metadata": {
        "id": "04BGif-QOKNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "rL8WClXcsY8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7fccfb-0210-439b-c898-11c70155778b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.65      0.61      0.63       575\n",
            "    negative       0.65      0.68      0.67       603\n",
            "\n",
            "    accuracy                           0.65      1178\n",
            "   macro avg       0.65      0.65      0.65      1178\n",
            "weighted avg       0.65      0.65      0.65      1178\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "id": "jqdv3a8osUH4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "outputId": "604e73fb-1a27-47f5-c234-e0847b105219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 725,
              "height": 516
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lDlD-S3rHTV"
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "from sklearn import metrics as m\n",
        "import json\n",
        "\n",
        "def json_metrics(file_name, prediction_model, metrics, df):\n",
        "    dictionary = {'Model': prediction_model,\n",
        "                  'Metrics': metrics,\n",
        "                  'Data': df.to_dict('records')}\n",
        "\n",
        "    if path.isfile(file_name): #file exist\n",
        "        with open(file_name) as fp:\n",
        "            listObj = json.load(fp)\n",
        "\n",
        "        listObj.append(dictionary)\n",
        "\n",
        "        with open(file_name, 'w') as json_file:\n",
        "            json.dump(listObj, json_file, indent=4)\n",
        "    else:\n",
        "        with open(file_name, 'w') as json_file:\n",
        "            json.dump([dictionary], json_file, indent=4)\n",
        "            \n",
        "def metrics(y_test, y_pred, target_names):\n",
        "    tn, fp, fn, tp = m.confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
        "    dict_confusion = {'True negative' : int(tn),\n",
        "          'False positive' : int(fp),\n",
        "          'False negative' : int(fn),\n",
        "          'True positive' : int(tp),\n",
        "          }\n",
        "    dict_report = m.classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
        "    return {**dict_confusion , **dict_report}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpah9SBLr2LY"
      },
      "outputs": [],
      "source": [
        " metric = metrics(y_test, y_pred, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsiL9kfCt0qU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "n_train = math.floor(0.9 * df_user_embeddings['label'].shape[0])\n",
        "df = df_user_embeddings[n_train:]\n",
        "n_train = math.floor(0.5*df['label'].shape[0])\n",
        "df = df[n_train:]\n",
        "\n",
        "df= pd.DataFrame({'sar_id': df['sar_id'].values, 'label': y_test, 'prediction': y_pred})\n",
        "\n",
        "json_metrics(\"/content/drive/MyDrive/SP/bert_beginning.json\", \"sentence BERT\", metric, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOX5rb9xLYmF"
      },
      "outputs": [],
      "source": [
        "#plt.plot(history['train_acc'], label='train accuracy')\n",
        "#plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "#plt.title('Training history')\n",
        "#plt.ylabel('Accuracy')\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.legend()\n",
        "#plt.ylim([0, 1]);"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-wqQfCdPk-yO",
        "r30ag7chkWz7",
        "3eb8Jc4qNd2J",
        "BEFm3ELt6OD6",
        "-uDCtSxd95V5"
      ],
      "name": "BERT_user_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "237bfdf55b344e78bb77758cd04b6acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16f9c82134ef4c4fa5636e9ec5ec2620",
              "IPY_MODEL_a94692d114a14c048b09b033b7be974e",
              "IPY_MODEL_e306d588ee5a4724996292c3f2a5bef1"
            ],
            "layout": "IPY_MODEL_971804bd27b14375a46a3b3a97bc133d"
          }
        },
        "16f9c82134ef4c4fa5636e9ec5ec2620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ad6349033b43b7bccb4b4a2e87cdc5",
            "placeholder": "​",
            "style": "IPY_MODEL_77b666be06df44118cda143aed2dafd6",
            "value": "Downloading: 100%"
          }
        },
        "a94692d114a14c048b09b033b7be974e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b95e4a2d50348caad1413270c9566d6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f372615774140d68722d690a99828b2",
            "value": 231508
          }
        },
        "e306d588ee5a4724996292c3f2a5bef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57aac605ed9b4bdc9028cdac53f902d2",
            "placeholder": "​",
            "style": "IPY_MODEL_f19c9189c9674760a9690b2cf955b832",
            "value": " 226k/226k [00:00&lt;00:00, 1.20MB/s]"
          }
        },
        "971804bd27b14375a46a3b3a97bc133d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ad6349033b43b7bccb4b4a2e87cdc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b666be06df44118cda143aed2dafd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b95e4a2d50348caad1413270c9566d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f372615774140d68722d690a99828b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57aac605ed9b4bdc9028cdac53f902d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19c9189c9674760a9690b2cf955b832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "875c6564c23c4c36be6c9b6564ae9133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06968769602b459aae89b9a4d9fcc5c8",
              "IPY_MODEL_a2a158d5f4a44df8becedb66020cefc3",
              "IPY_MODEL_020380022e2149e3851312018bbca75a"
            ],
            "layout": "IPY_MODEL_e3ea2795cadd4e26895c07b3c5f970ff"
          }
        },
        "06968769602b459aae89b9a4d9fcc5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e4d4406b6241a983eee07d5cfc106a",
            "placeholder": "​",
            "style": "IPY_MODEL_cc59c2d25a1a42eba83cbedcbdfa8271",
            "value": "Downloading: 100%"
          }
        },
        "a2a158d5f4a44df8becedb66020cefc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a352308384494aba9011dfb3627b9f99",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88d2c3c5b5214074a12286765be0ad70",
            "value": 112
          }
        },
        "020380022e2149e3851312018bbca75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c1404dfcfb494cad13a5d37d9ec918",
            "placeholder": "​",
            "style": "IPY_MODEL_73d182e6195a49f98e076ef559569a75",
            "value": " 112/112 [00:00&lt;00:00, 981B/s]"
          }
        },
        "e3ea2795cadd4e26895c07b3c5f970ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4e4d4406b6241a983eee07d5cfc106a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc59c2d25a1a42eba83cbedcbdfa8271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a352308384494aba9011dfb3627b9f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88d2c3c5b5214074a12286765be0ad70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51c1404dfcfb494cad13a5d37d9ec918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d182e6195a49f98e076ef559569a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ffb2ca2e6eb4d8c844ff8e17d984c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a76de41fe56a4ecb9268e0fc766a345b",
              "IPY_MODEL_53dbf68acece4adf901d7083ad22d897",
              "IPY_MODEL_b0627e57e0594cd681b6e58635457b1d"
            ],
            "layout": "IPY_MODEL_b827fffa9146419eae05e9046d812bf9"
          }
        },
        "a76de41fe56a4ecb9268e0fc766a345b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a327ca8f49a4078a6990147a1c30fea",
            "placeholder": "​",
            "style": "IPY_MODEL_36dd68ad6ebc4b01b21c76d87c96a320",
            "value": "Downloading: 100%"
          }
        },
        "53dbf68acece4adf901d7083ad22d897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24dda418930c4206a01081efcad2c6a2",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c510def81f954c0e9d4bc5dbf1ce12bc",
            "value": 350
          }
        },
        "b0627e57e0594cd681b6e58635457b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_911fc58a4777465a9f2d16f75349e7a5",
            "placeholder": "​",
            "style": "IPY_MODEL_693e5991f8f64949a0dc57375dc3f932",
            "value": " 350/350 [00:00&lt;00:00, 1.99kB/s]"
          }
        },
        "b827fffa9146419eae05e9046d812bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a327ca8f49a4078a6990147a1c30fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36dd68ad6ebc4b01b21c76d87c96a320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24dda418930c4206a01081efcad2c6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c510def81f954c0e9d4bc5dbf1ce12bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "911fc58a4777465a9f2d16f75349e7a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "693e5991f8f64949a0dc57375dc3f932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8412807347b4abcb1ceac36d3b4a3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ae86b8cf23b49e7aed9a6220bdec217",
              "IPY_MODEL_71ad58a7e75548369bd68a0d34d95401",
              "IPY_MODEL_be6358dce9f24a28bcfa36fb6e816e2f"
            ],
            "layout": "IPY_MODEL_619fbbf748c8499bb5fe84f36b9310af"
          }
        },
        "9ae86b8cf23b49e7aed9a6220bdec217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4861ef458e42aba65104315ddb33de",
            "placeholder": "​",
            "style": "IPY_MODEL_6ee2a79a7ce24ed1ba3589bfbb6c6591",
            "value": "Downloading: 100%"
          }
        },
        "71ad58a7e75548369bd68a0d34d95401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0078bdfa6b854dbaa899ad61e545603b",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e075cef02f54337b37dd0181c06b4cf",
            "value": 612
          }
        },
        "be6358dce9f24a28bcfa36fb6e816e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76f4b67a28b4f8da16e0e1d3609c619",
            "placeholder": "​",
            "style": "IPY_MODEL_04109be4e4a84c4e9e7310fea336fc23",
            "value": " 612/612 [00:00&lt;00:00, 15.1kB/s]"
          }
        },
        "619fbbf748c8499bb5fe84f36b9310af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4861ef458e42aba65104315ddb33de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ee2a79a7ce24ed1ba3589bfbb6c6591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0078bdfa6b854dbaa899ad61e545603b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e075cef02f54337b37dd0181c06b4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d76f4b67a28b4f8da16e0e1d3609c619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04109be4e4a84c4e9e7310fea336fc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62848be1d268400495f84f6276cd37b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a966adc18cc643e1b1192d1ad8ed95c6",
              "IPY_MODEL_a6328f0333514af485806319e3b1d4c7",
              "IPY_MODEL_30cbe335cbe14d4da065f3e0cd11f7d0"
            ],
            "layout": "IPY_MODEL_aa2dca0a32ee43748eeb2036997e5896"
          }
        },
        "a966adc18cc643e1b1192d1ad8ed95c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f7d341e20f4c9b924660d7bb84ca05",
            "placeholder": "​",
            "style": "IPY_MODEL_68fdd998d68349d48652471fe741f3c4",
            "value": "Downloading: 100%"
          }
        },
        "a6328f0333514af485806319e3b1d4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3cd1c748604d9c8e4e77b1da71e5c0",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03481f7f2da9468991c3f0403e3bde0a",
            "value": 90888945
          }
        },
        "30cbe335cbe14d4da065f3e0cd11f7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50e8b2b040e640da8db05acd01dc24a5",
            "placeholder": "​",
            "style": "IPY_MODEL_1b035aa0cdb14307bf294ae1afd3d0ec",
            "value": " 86.7M/86.7M [00:01&lt;00:00, 64.6MB/s]"
          }
        },
        "aa2dca0a32ee43748eeb2036997e5896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f7d341e20f4c9b924660d7bb84ca05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68fdd998d68349d48652471fe741f3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c3cd1c748604d9c8e4e77b1da71e5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03481f7f2da9468991c3f0403e3bde0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50e8b2b040e640da8db05acd01dc24a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b035aa0cdb14307bf294ae1afd3d0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}