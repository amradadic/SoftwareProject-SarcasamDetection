{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "user_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6PlwpV10pcDZ",
        "A-VPNeJhqkOK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### PREPROCESSING"
      ],
      "metadata": {
        "id": "6PlwpV10pcDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypYRg24Vpydt",
        "outputId": "518b72aa-db46-4973-ee43-be6e0a920947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 8.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=bf195021e1d8d0d10ad582d2987670939c71f28ffcbce644f2ac26de9ce2b65f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "string.punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "def remove_na_from_column(df, column_name):\n",
        "    df = df.dropna(subset = [column_name])\n",
        "    df = df.reset_index(drop = True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def fill_na_from_column(df, column_name):\n",
        "    df[column_name] = df[column_name].fillna('')\n",
        "\n",
        "    return df\n",
        "\n",
        "EMOJI_DESCRIPTION_SCRUB = re.compile(r':(\\S+?):')\n",
        "HASHTAG_BEFORE = re.compile(r'#(\\S+)')\n",
        "FIND_MENTIONS = re.compile(r'@(\\S+)')\n",
        "LEADING_NAMES = re.compile(r'^\\s*((?:@\\S+\\s*)+)')\n",
        "TAIL_NAMES = re.compile(r'\\s*((?:@\\S+\\s*)+)$')\n",
        "\n",
        "def preprocess_tweets(df, column_name='sar_text', keep_emoji = True): #column_name=tweet\n",
        "    df[column_name] = df[column_name].transform(func = process_tweet, keep_emoji=keep_emoji, keep_usernames=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_tweet(s, keep_emoji=True, keep_usernames=False):\n",
        "\n",
        "    s = s.lower()\n",
        "\n",
        "    #removing urls, htmls tags, etc\n",
        "    s = re.sub(r'https\\S+', r'', str(s))\n",
        "    s = re.sub(r'\\\\n', ' ', s)\n",
        "    s = re.sub(r'\\s', ' ', s)\n",
        "    s = re.sub(r'<br>', ' ', s)\n",
        "    s = re.sub(r'&amp;', '&', s)\n",
        "    s = re.sub(r'&#039;', \"'\", s)\n",
        "    s = re.sub(r'&gt;', '>', s)\n",
        "    s = re.sub(r'&lt;', '<', s)\n",
        "    s = re.sub(r'\\'', \"'\", s)\n",
        "\n",
        "    #removing stopwords\n",
        "    s = remove_stopwords(s)\n",
        "\n",
        "    #removing emojis\n",
        "    if keep_emoji:\n",
        "        s = emoji.demojize(s)\n",
        "    else:\n",
        "        emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "\n",
        "        s = emoj.sub(r'',s)\n",
        "\n",
        " #   s = re.sub(r\"\\\\x[0-9a-z]{2,3,4}\", \"\", s)\n",
        "\n",
        "    #removing hashtags\n",
        "    s = re.sub(HASHTAG_BEFORE, r'\\1!!', s)\n",
        "\n",
        "\n",
        "    #removing usernames\n",
        "\n",
        "    #removing just @sign\n",
        "    if keep_usernames:\n",
        "        s = ' '.join(s.split())\n",
        "\n",
        "        s = re.sub(LEADING_NAMES, r' ', s)\n",
        "        s = re.sub(TAIL_NAMES, r' ', s)\n",
        "\n",
        "        s = re.sub(FIND_MENTIONS, r'\\1', s)\n",
        "\n",
        "    #removing username completely\n",
        "    else:\n",
        "        s = re.sub(FIND_MENTIONS, r' ', s)\n",
        "    \n",
        "    #removing username tags - just in case ??\n",
        "    s = re.sub(re.compile(r'@(\\S+)'), r'@', s)\n",
        "    user_regex = r\".?@.+?( |$)|<@mention>\"    \n",
        "    s = re.sub(user_regex,\" @user \", s, flags=re.I)\n",
        "    \n",
        "    # Just in case -- remove any non-ASCII and unprintable characters, apart from whitespace  \n",
        "    s = \"\".join(x for x in s if (x.isspace() or (31 < ord(x) < 127)))\n",
        "    s = ' '.join(s.split())\n",
        "\n",
        "    return s\n",
        "\n",
        "def remove_punctiation(df, column_name='tweet'):\n",
        "    df[column_name] = df[column_name].transform(remove_punctuation)\n",
        "\n",
        "    return df\n",
        "    \n",
        "def remove_punctuation(text):\n",
        "    if(type(text)==float):\n",
        "        return text\n",
        "    \n",
        "    ans=\"\"  \n",
        "    for i in text:     \n",
        "        if i not in string.punctuation:\n",
        "            ans+=i    \n",
        "            \n",
        "    return ans\n",
        "\n",
        "def remove_nltk_stopwords(df, column_name='tweet') :\n",
        "    df[column_name] = df[column_name].transform(remove_nltk_stopwords_from_tweet)\n",
        "\n",
        "    return df\n",
        "    \n",
        "    \n",
        "def remove_nltk_stopwords_from_tweet(s):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(s)\n",
        "    tokens_without_sw = [word for word in word_tokens if not word in stop_words]\n",
        "    \n",
        "    s = (\" \").join(tokens_without_sw)\n",
        "    \n",
        "    return s  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0myJ-Sd8pbCU",
        "outputId": "ab979b44-55a5-447b-caf1-e604e01debd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###USER EMBBEDINGS"
      ],
      "metadata": {
        "id": "A-VPNeJhqkOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/SPIRS\")"
      ],
      "metadata": {
        "id": "KCpE8Xuw9AmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9047e7-5b96-482e-d915-65ca2b27f8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jkNzrKM_ZCx",
        "outputId": "6fa18ebc-b36b-4c82-e69b-f00dbd218a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spirs_history.zip\t SPIRS-sarcastic.csv\n",
            "SPIRS-non-sarcastic.csv  user_mentions2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#USER HISTORY"
      ],
      "metadata": {
        "id": "Z45NxNVMOUVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "import zipfile\n",
        "import csv\n",
        "from csv import DictWriter\n",
        "\n",
        "OUTPUT_FILE = 'user_embeddings_non_sarcastic_history.csv'\n",
        "INPUT_FILE = 'spirs_history/SPIRS-non-sarcastic-history.txt'\n",
        "TOKENS_NO = 250\n",
        "os.chdir(\"/content/drive/My Drive/SP\")\n",
        "csv_file = open(OUTPUT_FILE, 'a')\n",
        "dictwriter_object = DictWriter(csv_file, fieldnames=['user_id', 'history'])\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/SPIRS\")\n",
        "with zipfile.ZipFile('spirs_history.zip') as zip:\n",
        "    with zip.open(INPUT_FILE, mode='r') as file:\n",
        "      line = file.readline().split(b'\\t')\n",
        "      old_user_id = int(line[0])\n",
        "      sar_text = []\n",
        "      for line in file:\n",
        "        line = line.decode('UTF-8')\n",
        "        line = line.split('\\t')\n",
        "        user_id = int(line[0])\n",
        "        tweet_text = process_tweet(line[2]) \n",
        "\n",
        "        if (len(tweet_text)) == 0:\n",
        "          continue\n",
        "      \n",
        "        if user_id == old_user_id: \n",
        "          sar_text.append(tweet_text)\n",
        "        else:\n",
        "          sar_text = random.sample(sar_text, len(sar_text))\n",
        "          sentences = ''\n",
        "          count = 0\n",
        "          #count tokens, add tweets to embedding \n",
        "          for tweet in sar_text:\n",
        "            #tokens = tweet.split(' ')\n",
        "            tokens = nltk.word_tokenize(tweet)\n",
        "            count += len(tokens) + 1 #+1 because of [SEP]\n",
        "            if count <= TOKENS_NO:\n",
        "              sentences += tweet + ' [SEP] '\n",
        "            else:\n",
        "              count -= len(tokens)\n",
        "              sar_text = []\n",
        "              break\n",
        "\n",
        "          #if num of tokens is smaller than expected add padding\n",
        "          while count <= TOKENS_NO:\n",
        "            sentences += ' [PAD]'\n",
        "            count += 1\n",
        "\n",
        "          embedding = {'user_id': old_user_id, 'history': sentences}\n",
        "          dictwriter_object.writerow(embedding)\n",
        "\n",
        "          old_user_id = user_id\n",
        "          "
      ],
      "metadata": {
        "id": "mDAy-oaNf--8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONCAT USER HISTORY WITH TWEETS"
      ],
      "metadata": {
        "id": "uCzN2XrBMtYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader\n",
        "import csv\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/SPIRS\")\n",
        "\n",
        "INPUT_FILE = 'SPIRS-non-sarcastic.csv'\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "df = pd.DataFrame({'sar_user': np.array(df['sar_user']), 'sar_id': np.array(df['sar_id']), 'sar_text': np.array(df['sar_text'])})\n",
        "df = remove_na_from_column(df, 'sar_user')\n",
        "df = remove_na_from_column(df, 'sar_id')\n",
        "df = remove_na_from_column(df, 'sar_text')\n",
        "df = preprocess_tweets(df, 'sar_text')\n",
        "\n",
        "for i in range(len(df['sar_user'])):\n",
        "  user = df['sar_user'][i].split('|')\n",
        "  df['sar_user'][i] = int(user[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pmF-6m4dZBX",
        "outputId": "a8dc33fa-a010-4ab9-9109-0ed485c9973d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/SP\")\n",
        "file_history = open('user_embeddings_non_sarcastic_history.csv', 'r')\n",
        "csv_reader = reader(file_history)\n",
        "\n",
        "OUTPUT_FILE = 'user_embeddings_non_sarcastic_beginning.csv'\n",
        "file_write = open(OUTPUT_FILE, 'a')\n",
        "writer = csv.writer(file_write)\n",
        "writer.writerow(['user_id', 'sar_id', 'sar_text'])\n",
        "\n",
        "end = False   #if true add tweet to the end\n",
        "\n",
        "for row in csv_reader:\n",
        "  users_tweets = df[df['sar_user'] == int(row[0])]   #find tweets of user\n",
        "  sar_id = users_tweets['sar_id'].values\n",
        "  tweets = users_tweets['sar_text'].values\n",
        "  \n",
        "\n",
        "  for i in range(len(tweets)):\n",
        "    if end:\n",
        "      extended_tweet = '[CLS] ' + row[1] + ' ' + tweets[i]\n",
        "    else:\n",
        "      extended_tweet = '[CLS] ' + tweets[i] + ' [SEP] ' + row[1] \n",
        "    \n",
        "    writer.writerow([str(row[0]), str(sar_id[i]), extended_tweet])   #user_id, sar_id, tweet + history"
      ],
      "metadata": {
        "id": "963XS9XrMtFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}