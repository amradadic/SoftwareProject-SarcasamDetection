{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "User_embedding_averaging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9W3cp9SOAny",
        "outputId": "1a8f990d-aab4-4d60-fdcc-687af870b0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/SPIRS\")"
      ],
      "metadata": {
        "id": "HH2fSyRJOsqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fU29lA9OvZn",
        "outputId": "c090275d-b138-4081-e6d1-7852ed74097f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 19.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 65.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=0b7b14cf10df309c1e40072f6cf647d6f6e3a0156bdbfd68f05f63102c4d9b90\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 sentence-transformers-2.2.2 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "from csv import reader\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import csv\n",
        "import tensorflow"
      ],
      "metadata": {
        "id": "dQMMUY07O2dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMOJI_DESCRIPTION_SCRUB = re.compile(r':(\\S+?):')\n",
        "HASHTAG_BEFORE = re.compile(r'#(\\S+)')\n",
        "FIND_MENTIONS = re.compile(r'@(\\S+)')\n",
        "LEADING_NAMES = re.compile(r'^\\s*((?:@\\S+\\s*)+)')\n",
        "TAIL_NAMES = re.compile(r'\\s*((?:@\\S+\\s*)+)$')"
      ],
      "metadata": {
        "id": "z1wNBRW8O9OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import emoji"
      ],
      "metadata": {
        "id": "gCsUPPAEO-cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrH_Z80KPCP0",
        "outputId": "2fa45a70-e664-49fe-b193-550e18937ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 7.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=9960a101c509b0dc8115d3db74853825a4550977e331b8eaea2c9473b5d228aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function\n",
        "def process_tweet(s, keep_emoji=True, keep_usernames=False):\n",
        "\n",
        "  s = s.lower()\n",
        "\n",
        "  #removing urls, htmls tags, etc\n",
        "  s = re.sub(r'https\\S+', r'', str(s))\n",
        "  s = re.sub(r'\\\\n', ' ', s)\n",
        "  s = re.sub(r'\\s', ' ', s)\n",
        "  s = re.sub(r'<br>', ' ', s)\n",
        "  s = re.sub(r'&amp;', '&', s)\n",
        "  s = re.sub(r'&#039;', \"'\", s)\n",
        "  s = re.sub(r'&gt;', '>', s)\n",
        "  s = re.sub(r'&lt;', '<', s)\n",
        "  s = re.sub(r'\\'', \"'\", s)\n",
        "\n",
        "  #removing stopwords\n",
        "  s = remove_stopwords(s)\n",
        "\n",
        "  #removing emojis\n",
        "  if keep_emoji:\n",
        "      s = emoji.demojize(s)\n",
        "  else:\n",
        "      emoj = re.compile(\"[\"\n",
        "      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "      u\"\\U00002702-\\U000027B0\"\n",
        "      u\"\\U00002702-\\U000027B0\"\n",
        "      u\"\\U000024C2-\\U0001F251\"\n",
        "      u\"\\U0001f926-\\U0001f937\"\n",
        "      u\"\\U00010000-\\U0010ffff\"\n",
        "      u\"\\u2640-\\u2642\" \n",
        "      u\"\\u2600-\\u2B55\"\n",
        "      u\"\\u200d\"\n",
        "      u\"\\u23cf\"\n",
        "      u\"\\u23e9\"\n",
        "      u\"\\u231a\"\n",
        "      u\"\\ufe0f\"  # dingbats\n",
        "      u\"\\u3030\"\n",
        "                    \"]+\", re.UNICODE)\n",
        "\n",
        "      s = emoj.sub(r'',s)\n",
        "\n",
        " #   s = re.sub(r\"\\\\x[0-9a-z]{2,3,4}\", \"\", s)\n",
        "\n",
        "  #removing hashtags\n",
        "  s = re.sub(HASHTAG_BEFORE, r'\\1!!', s)\n",
        "\n",
        "\n",
        "  #removing usernames\n",
        "\n",
        "  #removing just @sign\n",
        "  if keep_usernames:\n",
        "      s = ' '.join(s.split())\n",
        "\n",
        "      s = re.sub(LEADING_NAMES, r' ', s)\n",
        "      s = re.sub(TAIL_NAMES, r' ', s)\n",
        "\n",
        "      s = re.sub(FIND_MENTIONS, r'\\1', s)\n",
        "\n",
        "  #removing username completely\n",
        "  else:\n",
        "      s = re.sub(FIND_MENTIONS, r' ', s)\n",
        "    \n",
        "  #removing username tags - just in case ??\n",
        "  s = re.sub(re.compile(r'@(\\S+)'), r'@', s)\n",
        "  user_regex = r\".?@.+?( |$)|<@mention>\"    \n",
        "  s = re.sub(user_regex,\" @user \", s, flags=re.I)\n",
        "  \n",
        "  # Just in case -- remove any non-ASCII and unprintable characters, apart from whitespace  \n",
        "  s = \"\".join(x for x in s if (x.isspace() or (31 < ord(x) < 127)))\n",
        "  s = ' '.join(s.split())\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "U-g-nsH7PGBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling user tweets\n",
        "file = None\n",
        "\n",
        "with zipfile.ZipFile('spirs_history.zip') as zip:\n",
        "  file = zip.open('spirs_history/SPIRS-non-sarcastic-history.txt', mode='r')\n",
        "\n",
        "dictionary = {}\n",
        "old_user = None\n",
        "sentences = []\n",
        "\n",
        "i = 0\n",
        "n_user_tweets = 0\n",
        "\n",
        "for line in file:\n",
        "\n",
        "    try:\n",
        "  \n",
        "      user_id, tweet_id, tweet = re.split(r'\\t+', line.decode('utf-8'))\n",
        "\n",
        "      if user_id in dictionary:\n",
        "        dictionary[user_id] = dictionary[user_id] + process_tweet(tweet) + '\\t'\n",
        "      else:\n",
        "        dictionary[user_id] = process_tweet(tweet) + '\\t'\n",
        "\n",
        "    except:\n",
        "\n",
        "      print(re.split(r'\\t+', line.decode('utf-8')))\n",
        "\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "50e8gbeCPLrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc7b3a0-2801-4163-9442-fcbc036acf3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['301817957', '1325115154195996673', '@RobAdamsFL @SportyMama @MillerMitsu @DSofia21 @ernesto3311 @HardBodyCraig @mlandres12 @RomanGarciaJr @PeteTheStorm @marlid83 @CutlerRidgeLAZ @ChinoLutz @WMGarbageman @dpburnette @Ballgameboss @STEM08 @mojicapr @razincane83 @HugeHoopsFan @ColtenMetzger @ofcourseimajew @KING_G_GILMORE @bigpunisher305 @RB4420 @KappaCane @RobertPerera5 @ChicoHull @chadmch @4feldman @SmallsLaw @lockhart_jesse @cfhell31 @Raymond3633 @HeavyFaithSteve @Romancane @FatherOfKane @gatorfan960608 @TwentyER @youfit @grinders @madiadams_ @JacOnMac @Apple @Maureen52375 @MiamiHEAT @FSUFootball BBR370876756', '1334534491063390208', '@KateMcLoughney @BobbiPeach58 I always have some in\\r\\n']\n",
            "['818041150066032641', '1290739498741952512', 'RT @CHARMINGMYG: please spread this https://t.co/qdq34bSDsj624692430', '1334671088559775744', 'RT @Eric_A_Stanley: It’s not that people don’t understand “defund the police” it’s that they want the antiblack, ablest, homo/transphobic,…\\r\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating tweets representation\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "from csv import writer\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "model = model.to(device)\n",
        "\n",
        "out_file = 'user_embeddings_nonsar.csv'\n",
        "\n",
        "with open(out_file, 'a') as f:\n",
        "\n",
        "  for key in dictionary:\n",
        "\n",
        "    tweets = dictionary[key].split('\\t')\n",
        "    tweets_length = len(tweets)\n",
        "\n",
        "    embedding = None\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i*128 < tweets_length :\n",
        "\n",
        "      if i*128 + 128 < tweets_length :\n",
        "        tweet = tweets[i*128 : i*128 + 128]\n",
        "      else :\n",
        "        tweet = tweets[i*128 : tweets_length-1]\n",
        "\n",
        "      if len(tweet) == 0:\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "      encoded_input = tokenizer(tweet, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "      # Compute token embeddings\n",
        "      with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "      # Perform pooling\n",
        "      sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "      # Normalize embeddings\n",
        "      sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "\n",
        "      #mean - [128, 384] -> [1, 384]\n",
        "      if embedding is None:\n",
        "        embedding = torch.mean(torch.Tensor.cpu(sentence_embeddings), 0).numpy()\n",
        "\n",
        "      else :\n",
        "        embedding += torch.mean(torch.Tensor.cpu(sentence_embeddings), 0).numpy()\n",
        "\n",
        "      i += 1\n",
        "    \n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([key, np.array_str((1/i)*embedding, max_line_width=np.inf)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX9sh2XBPkZi",
        "outputId": "366838e0-b79c-434f-c22c-6a6dc31be35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"
          ]
        }
      ]
    }
  ]
}