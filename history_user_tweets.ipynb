{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398df8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.10) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#SAMPLING USER TWEETS FROM SPIRS HISTORY AND CREATING THEIR REPRESENTATION\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from csv import reader\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35195b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    if(type(text)==float):\n",
    "        return text\n",
    "    \n",
    "    ans=\"\"  \n",
    "    for i in text:     \n",
    "        if i not in string.punctuation:\n",
    "            ans+=i    \n",
    "            \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72c112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOJI_DESCRIPTION_SCRUB = re.compile(r':(\\S+?):')\n",
    "HASHTAG_BEFORE = re.compile(r'#(\\S+)')\n",
    "FIND_MENTIONS = re.compile(r'@(\\S+)')\n",
    "LEADING_NAMES = re.compile(r'^\\s*((?:@\\S+\\s*)+)')\n",
    "TAIL_NAMES = re.compile(r'\\s*((?:@\\S+\\s*)+)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e388e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ec2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(s, keep_emoji=True, keep_usernames=False):\n",
    "\n",
    "    s = s.lower()\n",
    "\n",
    "    #removing urls, htmls tags, etc\n",
    "    s = re.sub(r'https\\S+', r'', str(s))\n",
    "    s = re.sub(r'\\\\n', ' ', s)\n",
    "    s = re.sub(r'\\s', ' ', s)\n",
    "    s = re.sub(r'<br>', ' ', s)\n",
    "    s = re.sub(r'&amp;', '&', s)\n",
    "    s = re.sub(r'&#039;', \"'\", s)\n",
    "    s = re.sub(r'&gt;', '>', s)\n",
    "    s = re.sub(r'&lt;', '<', s)\n",
    "    s = re.sub(r'\\'', \"'\", s)\n",
    "\n",
    "    #removing stopwords\n",
    "    s = remove_stopwords(s)\n",
    "\n",
    "    #removing emojis\n",
    "    if keep_emoji:\n",
    "        s = emoji.demojize(s)\n",
    "    else:\n",
    "        emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                    \"]+\", re.UNICODE)\n",
    "\n",
    "        s = emoj.sub(r'',s)\n",
    "\n",
    "    #   s = re.sub(r\"\\\\x[0-9a-z]{2,3,4}\", \"\", s)\n",
    "\n",
    "    #removing hashtags\n",
    "    s = re.sub(HASHTAG_BEFORE, r'\\1!!', s)\n",
    "\n",
    "\n",
    "    #removing usernames\n",
    "\n",
    "    #removing just @sign\n",
    "    if keep_usernames:\n",
    "        s = ' '.join(s.split())\n",
    "\n",
    "        s = re.sub(LEADING_NAMES, r' ', s)\n",
    "        s = re.sub(TAIL_NAMES, r' ', s)\n",
    "\n",
    "        s = re.sub(FIND_MENTIONS, r'\\1', s)\n",
    "\n",
    "    #removing username completely\n",
    "    else:\n",
    "        s = re.sub(FIND_MENTIONS, r' ', s)\n",
    "    \n",
    "    #removing username tags - just in case ??\n",
    "    s = re.sub(re.compile(r'@(\\S+)'), r'@', s)\n",
    "    user_regex = r\".?@.+?( |$)|<@mention>\"    \n",
    "    s = re.sub(user_regex,\" @user \", s, flags=re.I)\n",
    "\n",
    "    # Just in case -- remove any non-ASCII and unprintable characters, apart from whitespace  \n",
    "    s = \"\".join(x for x in s if (x.isspace() or (31 < ord(x) < 127)))\n",
    "    s = ' '.join(s.split())\n",
    "    \n",
    "    s = remove_punctuation(s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e933f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "users1 = pd.read_csv('split/x_train.csv')\n",
    "users2 = pd.read_csv('split/x_val.csv')\n",
    "users3 = pd.read_csv('split/x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad902822",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "\n",
    "for i in range(len(users1['sar_user'])):\n",
    "    user = users1['sar_user'][i].split('|')\n",
    "    users.append(int(user[-1]))\n",
    "    \n",
    "for i in range(len(users2['sar_user'])):\n",
    "    user = users2['sar_user'][i].split('|')\n",
    "    users.append(int(user[-1]))\n",
    "    \n",
    "for i in range(len(users3['sar_user'])):\n",
    "    user = users3['sar_user'][i].split('|')\n",
    "    users.append(int(user[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d2fc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23983"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08741b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate users\n",
    "users = list(dict.fromkeys(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0d6eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22952"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee844f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling tweets\n",
    "file = None\n",
    "\n",
    "with zipfile.ZipFile('/../../../data/sarcasm_data/spirs_history.zip') as zip:\n",
    "    file = zip.open('spirs_history/SPIRS-non-sarcastic-history.txt', mode='r')\n",
    "\n",
    "tweets_array = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "user_tweets = []\n",
    "old_user = -1\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    try:\n",
    "\n",
    "        user_id, tweet_id, tweet = re.split(r'\\t+', line.decode('utf-8'))\n",
    "\n",
    "        if int(user_id) in users:\n",
    "\n",
    "            if i == 0:\n",
    "                old_user = user_id\n",
    "\n",
    "\n",
    "            #new user\n",
    "            if user_id != old_user:\n",
    "\n",
    "                if len(user_tweets) > 100 :\n",
    "                    user_tweets = random.sample(user_tweets, 100)\n",
    "\n",
    "                for t in user_tweets:\n",
    "                    tweets_array.append(t)\n",
    "\n",
    "                user_tweets = []\n",
    "                old_user = user_id\n",
    "\n",
    "            else:\n",
    "                user_tweets.append([tweet_id, process_tweet(tweet), user_id])\n",
    "\n",
    "    except:\n",
    "        print(re.split(r'\\t+', line.decode('utf-8')))\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13c23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tweets_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d635b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "cuda:0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#creating tweets representation\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "from csv import writer\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(2)\n",
    "print(torch.cuda.current_device())\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "\n",
    "out_file = '/../../../data/sarcasm_data/user_embeddings_3_final.csv'\n",
    "\n",
    "with open(out_file, 'a') as f:\n",
    "\n",
    "    length = len(tweets_array)\n",
    "    i = 0\n",
    "    \n",
    "    while i < length:\n",
    "        \n",
    "        if i + 256 < length :\n",
    "            temp = tweets_array[i : i + 256]\n",
    "        else :\n",
    "            temp = tweets_array[i : length]\n",
    "            \n",
    "        tweets=list(np.reshape(temp, (len(temp),3))[:,1])\n",
    "        \n",
    "        #row[0] - tweet_id, row[1] - tweet, row[2] - user_id/label\n",
    "\n",
    "        encoded_input = tokenizer(tweets, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "        final_tensor = torch.Tensor.cpu(sentence_embeddings)\n",
    "\n",
    "        writer = csv.writer(f)\n",
    "    \n",
    "        for j in range(0,len(temp)):\n",
    "            tweet_id = temp[j][0]\n",
    "            embedding = final_tensor[j].numpy()\n",
    "            user_id = temp[j][2]\n",
    "            writer.writerow([tweet_id, np.array_str(embedding, max_line_width=np.inf), user_id])\n",
    "\n",
    "        if i % 102400 == 0:\n",
    "            print(i)\n",
    "        i += 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691acb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
