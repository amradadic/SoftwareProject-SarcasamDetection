{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r30ag7chkWz7"
      },
      "source": [
        "#Data preprocessing - ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT8BX9Ch5rPP",
        "outputId": "4f863472-e17c-47d7-c72f-f1c9be3faf8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "!%%cache\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/SP\")\n",
        "\n",
        "df_user_embeddings_sarcastic = pd.read_csv('/content/drive/MyDrive/SP/user_embeddings_sarcastic_beginning_50.csv')\n",
        "df_user_embeddings_non_sarcastic = pd.read_csv('/content/drive/MyDrive/SP/user_embeddings_non_sarcastic_beginning_50.csv')\n",
        "\n",
        "df_user_embeddings_sarcastic['label'] = 1\n",
        "df_user_embeddings_non_sarcastic['label'] = 0\n",
        "\n",
        "df_user_embeddings = pd.concat([df_user_embeddings_sarcastic, df_user_embeddings_non_sarcastic], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDz01CG81fPu"
      },
      "outputs": [],
      "source": [
        "x_train = pd.read_csv('/content/drive/MyDrive/SP/split/x_train.csv')\n",
        "y_train = pd.read_csv('/content/drive/MyDrive/SP/split/y_train.csv')\n",
        "train = pd.concat([x_train, y_train], axis=1, join='inner') \n",
        "\n",
        "x_test = pd.read_csv('/content/drive/MyDrive/SP/split/x_test.csv')\n",
        "y_test = pd.read_csv('/content/drive/MyDrive/SP/split/y_test.csv')\n",
        "test = pd.concat([x_test, y_test], axis=1, join='inner') \n",
        "\n",
        "x_val = pd.read_csv('/content/drive/MyDrive/SP/split/x_val.csv')\n",
        "y_val = pd.read_csv('/content/drive/MyDrive/SP/split/y_val.csv')\n",
        "val = pd.concat([x_val, y_val], axis=1, join='inner') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Buh6Rft3UH7"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/My Drive/SP/train_test_val\")\n",
        "data = pd.DataFrame(columns=['user_id', 'sar_id', 'sar_text', 'label'])\n",
        "for i in range(len(train['sar_id'])):\n",
        "  temp = df_user_embeddings[df_user_embeddings['sar_id']==train.loc[i].at['sar_id']]\n",
        "  if len(temp) != 0:\n",
        "    data = data.append(temp, ignore_index=True)\n",
        "data.to_csv('train_user_embeddings_beginning_50.csv')\n",
        "\n",
        "data = pd.DataFrame(columns=['user_id', 'sar_id', 'sar_text', 'label'])\n",
        "\n",
        "for i in range(len(test['sar_id'])):\n",
        "  temp = df_user_embeddings[df_user_embeddings['sar_id']==test.loc[i].at['sar_id']]\n",
        "  if len(temp) != 0:\n",
        "    data = data.append(temp, ignore_index=True)\n",
        "data.to_csv('test_user_embeddings_beginning_50.csv')\n",
        "\n",
        "data = pd.DataFrame(columns=['user_id', 'sar_id', 'sar_text', 'label'])\n",
        "\n",
        "for i in range(len(val['sar_id'])):\n",
        "  temp = df_user_embeddings[df_user_embeddings['sar_id']==val.loc[i].at['sar_id']]\n",
        "  if len(temp) != 0:\n",
        "    data = data.append(temp, ignore_index=True)\n",
        "data.to_csv('val_user_embeddings_beginning_50.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqQfCdPk-yO"
      },
      "source": [
        "#Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlV1Au7qE8xH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCd6mm5S4Z6U",
        "outputId": "3533f3f3-e960-4201-ebfb-0af1826e2178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.7/dist-packages (3.4.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from rpy2) (1.5.1)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from rpy2) (1.15.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from rpy2) (2.11.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from rpy2) (2022.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->rpy2) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "!pip install rpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGxc0tQ1wg_K",
        "outputId": "78a684d8-b739-44c7-c148-26c652d9845c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrJxCC3SwBhO",
        "outputId": "895da9da-7eb8-4b9d-b971-c2f36d70432b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 31.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 31.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "!pip install -qq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh8Z-6gRwjZ_",
        "outputId": "f107bedf-dc10-4b3e-aea8-f2cd251e15ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: fg: no job control\n",
            "Python implementation: CPython\n",
            "Python version       : 3.7.13\n",
            "IPython version      : 5.5.0\n",
            "\n",
            "numpy       : 1.21.6\n",
            "pandas      : 1.3.5\n",
            "torch       : 1.12.0+cu113\n",
            "transformers: 4.20.1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!%%cache\n",
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7LUN3WHOoPx"
      },
      "source": [
        "#Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7NShCKkOqI5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "!%%cache\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive\")\n",
        "\n",
        "df_train = pd.read_csv('')\n",
        "df_test = pd.read_csv('')\n",
        "df_val = pd.read_csv('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eb8Jc4qNd2J"
      },
      "source": [
        "#Setup and Confing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAwQMmKINaSX"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzkHiBOpwARW"
      },
      "outputs": [],
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFSN4CiwxINU"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6g3gHfy2JpF"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4kB5OeW0j6n"
      },
      "outputs": [],
      "source": [
        "class SarcasticTweetsDataset(Dataset):\n",
        "\n",
        "  def __init__(self, sar_texts, targets, tokenizer, max_len):\n",
        "    self.sar_texts = sar_texts\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sar_texts)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    sar_text = str(self.sar_texts[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      sar_text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation=True   #bez ovog\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'sar_text': sar_text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dhjGDcd1SI0"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = SarcasticTweetsDataset(\n",
        "    sar_texts=df.sar_text.to_numpy(),\n",
        "    targets=df.label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOuuOGPi1M84"
      },
      "outputs": [],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBV-lpwf1ifV"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 512\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpyy5TB_1x_S"
      },
      "outputs": [],
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpNze8ZP1zSS"
      },
      "outputs": [],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEFm3ELt6OD6"
      },
      "source": [
        "#Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4gQhe40B9xE"
      },
      "outputs": [],
      "source": [
        "class SarcasmClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    ob = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(ob.pooler_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZn6lkIo9ey_"
      },
      "outputs": [],
      "source": [
        "class_names = ['positive', 'negative']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVkEGDez9OH5"
      },
      "outputs": [],
      "source": [
        "model = SarcasmClassifier(len(class_names))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvA4gmEm9uAs"
      },
      "outputs": [],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPU-1Ki_9xrJ"
      },
      "outputs": [],
      "source": [
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uDCtSxd95V5"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTlmpyrb9yu5"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApnUtL4-B-nY"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE-7PLDjCAY3"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NakwLrugCJ5X"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM8MHtF0UUZH"
      },
      "outputs": [],
      "source": [
        "tmp = history['train_acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWa2Y3BFLij4"
      },
      "outputs": [],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHhrhEdkLs4m"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  sar_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"sar_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      sar_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return sar_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M-W2I5TL7XK"
      },
      "outputs": [],
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7IoWLAxDCUI"
      },
      "outputs": [],
      "source": [
        "history_train_acc = torch.Tensor.cpu(torch.tensor(history['train_acc']))\n",
        "history_val_acc = torch.Tensor.cpu(torch.tensor(history['val_acc']))\n",
        "\n",
        "plt.plot(history_train_acc, label='train accuracy')\n",
        "plt.plot(history_val_acc, label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04BGif-QOKNg"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FILE = ''\n",
        "MODEL_SAVE = ''"
      ],
      "metadata": {
        "id": "gBdNZl8ZenSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, MODEL_SAVE)"
      ],
      "metadata": {
        "id": "a5RpIJBhRAIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL8WClXcsY8f"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqdv3a8osUH4"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpah9SBLr2LY"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "from utils.metrics import metrics, json_metrics\n",
        "\n",
        "metric = metrics(y_test, y_pred, class_names)\n",
        "\n",
        "df= pd.DataFrame({'sar_id': df_test['sar_id'].values, 'label': y_test, 'prediction': y_pred})\n",
        "\n",
        "json_metrics(OUTPUT_FILE, \"sentence BERT\", 'Priming - 100 tokens, tweet at the beginning', metric, df)  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "r30ag7chkWz7",
        "-wqQfCdPk-yO",
        "_7LUN3WHOoPx",
        "3eb8Jc4qNd2J",
        "i6g3gHfy2JpF",
        "BEFm3ELt6OD6",
        "-uDCtSxd95V5",
        "04BGif-QOKNg"
      ],
      "name": "BERT_User_Embedding_Priming.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}